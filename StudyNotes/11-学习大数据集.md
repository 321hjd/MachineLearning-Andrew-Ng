> 应用于大数据集较为合适的两种算法：随机梯度下降 和 mini-batch 梯度下降。

### 一、随机梯度下降(stochastic gradient descent)

> 由于**之前**实现梯度下降时，每次迭代都需要考虑**所有样本**，因此也称为**批量梯度下降(batch gradient descent)**。如 $\theta_j := \theta_j-\alpha\frac{1}{m}\sum_\limits{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$ ，其中第二项包含所有样本。对于大数据集，这种方法并不适用。

随机梯度下降与批量梯度下降的区别在于，其迭代时不需要对整个样本进行求和，而是仅利用一个训练样本就可以进行参数的迭代优化。

算法过程：

1. 随机对训练数据集排序

2. Repeat {

   for $i: =1,...,m$ {

   $\theta_j:=\theta_j-\alpha(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j$  

   （for every $j=0,...,n$）

   ​	}

   }

   > 这里算法的优势在于，由于数据集很大（m很大），并且for循环内每一次迭代都在对参数 $\theta$ 进行优化，在一次完整的for循环内可能就已经得到了较好的参数，因此性能优于原本的批量梯度下降算法。正常来说repeat 1~10次是合理的，这取决于样本大小。

### 二、Mini-batch 梯度下降

> 思想：随机梯度下降算法的思想是每次迭代使用 1 个样本，Mini-batch 梯度下降的思想是每次迭代利用 b 个样本。

算法过程：

1. 假设 $b=10,m=1000$ 

2. Repeat {

   for $i=1,11,21,31,\ldots,991$ {

   $\theta_j:=\theta_j-\alpha\frac{1}{10}\sum_\limits{k=i}^{i+9}(h_\theta(x^{(k)})-y^{(k)})x^{(k)}_j$ 

   （for every $j=0,...,n$）

   ​	}

   }

   > 合适的向量化计算可以并行地实现算法，当且仅当此时 Mini-batch 算法优于随机梯度下降算法。

### 三、算法收敛性检验

**批量梯度下降**

> 画出代价函数 $J_{train}(\theta)=\frac{1}{2m}\sum_\limits{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$ 关于迭代次数的函数图像。

**随机梯度下降**

> 在每次更新 $\theta$ 前计算代价函数 $cost(x^{(i)},y^{(i)})=\frac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2$ 。然后每 1000（或者其它）次画一次平均 $cost$ 值，随着算法运行，同样将获得一个随迭代次数变化的代价函数值图像。
>
> **注意**：
>
> 1. 梯度下降并不是一个代价函数一直下降的过程，而是振荡下降，因此适当调低学习速率 $\alpha$ ，可能获得一个更优的结果，但相应收敛速度变慢。
> 2. 若学习曲线下降缓慢（收敛慢）或者振荡幅度大，则应该提高选择计算代价函数平均值的样本数量。
> 3. 若得到的学习曲线是增大的（发散），应该设置较小的学习速率 $\alpha$ 。
> 4. 一般情况下梯度下降的学习速率是不变的，但是为了提高算法的效果，也可以设置一个随迭代增加而逐渐减小的学习速率：$\alpha=\frac{const}{iterationNumber+const}$ 。

### 四、在线学习机制


> 对于具有**连续数据流**的场景（如在线运行的网站，拥有不断涌入注册的用户）可以利用在线学习机制，利用新增加用户的偏好改进算法的性能。即训练数据集不固定。**是随机梯度下降算法的变式。** 

例：产品搜索——点击率预测学习问题（CTR预测，click through rate）

有一个在线卖手机的商店，用户可以搜索相应的手机。假设店里有100部手机，每次搜索会返回10个结果。可以构建一个特征向量 $x$ ，代表用户搜索词与手机描述相匹配的程度等等，我们需要估计出用户点击每个手机链接的概率 $p(y=1|x;\theta)$ ，并用 $y=1$ 表示用户点击该链接，否则为 $y=0$ 。最后我们需要为用户显示10个最匹配的手机链接（用户最可能会点击的链接）。

对于这类问题，在线学习机制就是不断地为新用户展示为其推荐的10个最可能点击的链接，得到10个样本，并对这10个样本运行10步梯度下降算法来更新参数，然后丢弃这些样本，然后继续对进入的新用户流进行展示和学习。不断循环……类似的还有新闻抓取网站、招聘网站以及协同过滤推荐等等。

### 五、减少映射（MapReduce）与数据并行（data parallelism）

> 适用于数据过多以至于无法在单机上运行的大规模机器学习场景。只要算法能够表示为对训练集的求和，就可以使用MapReduce。

**Map-reduce** 

![](https://cdn.jsdelivr.net/gh/321hjd/ImageBed/MachineLearning/LargeDataSet/MapReduce-2.jpg)

![](https://cdn.jsdelivr.net/gh/321hjd/ImageBed/MachineLearning/LargeDataSet/MapReduce-1.jpg)

基于上图所示过程，代价函数及其偏导计算，都可以分解至多个机器进行并行运算，然后整合。（对于多核计算机，即便是同一计算机，也可以利用MapReduce进行并行计算）
